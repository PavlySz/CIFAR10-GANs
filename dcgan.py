'''Deep Convolutional Generative Adverserail Netowrks'''

# Import libraries
import torch

import torch.nn as nn
import torch.nn.parallel
import torch.optim as optimizer

import torch.utils.data
from torch.autograd import Variable

import torchvision.datasets as datasets
import torchvision.utils as tvutils
import torchvision.transforms as tvtransforms

import os

# Make sure Pytorch uses GPU
device = torch.device('cuda')
dtype = torch.cuda.FloatTensor


# Hyperparameters
BATCH_SIZE = 64
IMAGE_SIZE = 64     # set the size of each generated image to (64, 64)
N_EPOCHS = 10

# Transformations
transforms = tvtransforms.Compose([
    tvtransforms.Scale(IMAGE_SIZE),
    tvtransforms.ToTensor(),
    tvtransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Load dataset
# a. Download dataset to the './data' folder
# b. Apply the transformations specified above
print('[INFO] Loading dataset')
dataset = datasets.CIFAR10(root='data', download=True, transform=transforms)

# c. Load the dataset from './data' bacth by batch
data_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,
                                          shuffle=True)

# Initialize weights
def init_weights(neural_net):
    '''
    Initialize the neural network weights using uniform initialization
    Arguments:
        * neural_net: the neural netowrk to initialize the weights of
    '''

    # Get the name of the layer
    class_name = neural_net.__class__.__name__

    # For every layer in the netowrk
    if class_name.find('Conv') != -1:
        # Initialize the Conv layers uniformally
        neural_net.weight.data.normal_(0.0, 0.02)  # ...normal_(mean, std)

    elif class_name.find('BatchNorm') != -1:
        # Initialize the BatchNorm layers uniforamlly
        neural_net.weight.data.normal_(1.0, 0.02)

        # Fill the biases with 0
        neural_net.bias.data.fill_(0)


# Creating the Genrator class
class Generator(nn.Module):
    '''Generator, ay?'''
    def __init__(self):
        '''Genertaor architecture'''
        super(Generator, self).__init__()
        self.gen_model = nn.Sequential(
            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),  # 100 => size of input vector noise
            nn.BatchNorm2d(512),
            nn.ReLU(True),

            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(True),

            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(True),

            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(True),

            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False), # out_channels = 3 (RGB)
            nn.Tanh()
        )
        self.gen_model.cuda()

    def forward(self, input_noise):
        '''
        Forward propagates the signal inside of the Gnerator
        Argumets:
            * input_noise: noise random vector of size 100
        Returns:
            * output: output of the generator; a fake image
        '''
        output = self.gen_model(input_noise)
        return output

# Creating the Gen
print('[INFO] Creating Generator')
generator = Generator()
generator.apply(init_weights)   # Initializing Gen weights


# Creating the Discriminator class
class Discriminator(nn.Module):
    '''Discriminator. What you gonna do, motherfucker?'''
    def __init__(self):
        '''Defining the discriminator architecture'''

        # Initializing the Module class
        super(Discriminator, self).__init__()
        self.disc_model = nn.Sequential(
            nn.Conv2d(3, 64, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.02, True),

            nn.Conv2d(64, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, True),

            nn.Conv2d(128, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, True),

            nn.Conv2d(256, 512, 4, 2, 1, bias=False),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, True),

            nn.Conv2d(512, 1, 4, 1, 0, bias=False), # output = 1 channel (vector of 0 or 1)
            nn.Sigmoid()
        )
        self.disc_model.cuda()


    def forward(self, input_image):
        '''
        Forward propagates the signal inside of the Generator
        Argumets:
            * input_image: image generated by the Generator
        Returns:
            * output: output of the discrminator; a vector of (0, 1)
                      corresponding to whether the image generated by the Gen
                      is close to the real one or not
        '''
        output = self.disc_model(input_image)
        return output.view(-1)  # flatten the putput of the convolution


# Creating the Discriminator and itializing its weights
print('[INFO] Creating Discriminator')
discriminator = Discriminator()
discriminator.apply(init_weights)


# Training DCGAN
# a. Training the Discriminator with a real image
# b. Training the Disciriminator with a fake image

# Loss function
criterion = nn.BCELoss()

# Genertaor optimizer
gen_opt = optimizer.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))

# Discriminator optimizer
disc_opt = optimizer.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))


# Creating necessary directory
dirs = ['results', 'results/real', 'results/fake']
for directory in dirs:
    if not os.path.exists(directory):
        os.mkdir(directory)


# Big fucking training loop
print('[INFO] Started training...')
for epoch in range(N_EPOCHS+1):
    # Loop over the data mini-batches generated by the data_loader
    for i, data_batch in enumerate(data_loader, 0):
        # a. Training the Discriminator
        # a.0. Clear out the graients(?)
        discriminator.zero_grad()

        # a.1. Train the Discriminator with real images
        # Get a batch of real images
        real_images, _ = data_batch    # _ => is the target of each image, which we don't care about
        input_batch = Variable(real_images).type(dtype)

        # Define the target
        # Member that 1 corresponds to real images and 0 correspons to fake ones
        target_real = Variable(torch.ones(input_batch.size()[0])).type(dtype) # this is the batch size... yup...
        # print(f'input_batch.size()[0]: {input_batch.size()[0]}')    # Guess what? It's 64... Jesus fucking Christ

        # Feed the batch to the Disc
        y_train_pred_real = discriminator(input_batch)

        # Calcuate the error
        error_disc_real = criterion(y_train_pred_real, target_real)

        # a.2. Train the Discriminator with fake images
        # Generate fake images
        # Generate a noise vector of size 100
        # More like 100 feature maps each of size 1x1
        noise_vector = Variable(torch.randn(input_batch.size()[0], 100, 1, 1)).type(dtype)

        # Use the Gen to generate a batch of fake images from the noise_vectors
        fake_images = generator(noise_vector)

        # Target
        target_fake = Variable(torch.zeros(input_batch.size()[0])).type(dtype)

        # Train the Disc on fake images
        y_train_pred_fake = discriminator(fake_images.detach())

        # Calculate the error
        error_disc_fake = criterion(y_train_pred_fake, target_fake)

        # a.3. Back-prop the total error
        # Calculate the total error
        error_disc_total = error_disc_fake + error_disc_real

        # Back-prop the total error
        error_disc_total.backward()

        # Use Adam to update the weights
        disc_opt.step()


        # b. Training the Generator
        # Zero out the gradients
        generator.zero_grad()

        # Forward propagate the fake generated images into the Dscriminator to get the prediction
        y_train_gen = discriminator(fake_images)

        # Target if the target_real; the Gen wants the Disc to think that
        # the images generated by it are real
        error_gen = criterion(y_train_gen, target_real)

        # Back-prop the error to the GNN
        error_gen.backward()
        gen_opt.step()


        # Printing shit
        print(f'Epoch [{epoch}/{N_EPOCHS}] Step [{i}/{len(data_loader)}]', end=' ')

        print(f'LossDR: {error_disc_real.item():.3f}', end=' ')
        print(f'LossDF: {error_disc_fake.item():.3f}', end=' ')
        print(f'LossDT: {error_disc_total.item():.3f}', end=' ')

        print(f'LossG: {error_gen.item():.3f}')

        # Save real and fake images every 100 steps
        if i % 100 == 0:
            print('[INFO] Saving images in ./results')
            tvutils.save_image(real_images, 
                               f'./results/real/real_samples_epoch_{epoch:03d}_batch_{i:03d}.png', 
                               normalize=True)

            fake_images_2 = generator(noise_vector)

            tvutils.save_image(fake_images_2.data, 
                               f'./results/fake/fake_samples_epoch_{epoch:03d}_batch_{i:03d}.png',
                               normalize=True)

print('DONE!')
